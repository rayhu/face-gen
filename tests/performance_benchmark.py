#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
è¯¦ç»†æ¯”è¾ƒ MPS å’Œ CPU çš„æ€§èƒ½å·®å¼‚
"""

import torch
import time
import numpy as np

def benchmark_matrix_operations():
    """åŸºå‡†æµ‹è¯•çŸ©é˜µè¿ç®—"""
    print("ğŸ§® çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„çŸ©é˜µ
    sizes = [500, 1000, 2000, 3000]
    
    for size in sizes:
        print(f"\nğŸ“Š çŸ©é˜µå¤§å°: {size}x{size}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(size, size)
        y = torch.randn(size, size)
        
        # CPU æµ‹è¯•
        start_time = time.time()
        z_cpu = torch.mm(x, y)
        cpu_time = time.time() - start_time
        
        # MPS æµ‹è¯•
        if torch.backends.mps.is_available():
            x_mps = x.to('mps')
            y_mps = y.to('mps')
            
            start_time = time.time()
            z_mps = torch.mm(x_mps, y_mps)
            mps_time = time.time() - start_time
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = cpu_time / mps_time
            print(f"  CPU è€—æ—¶: {cpu_time:.3f}ç§’")
            print(f"  MPS è€—æ—¶: {mps_time:.3f}ç§’")
            print(f"  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1:
                print(f"  âœ… MPS æ›´å¿«")
            else:
                print(f"  âš ï¸  CPU æ›´å¿«")
        else:
            print("  âŒ MPS ä¸å¯ç”¨")

def benchmark_model_inference():
    """åŸºå‡†æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("\nğŸ¤– æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„æ¨¡å‹
    model_sizes = [
        (100, 50),
        (500, 200),
        (1000, 500),
        (2000, 1000)
    ]
    
    for input_size, output_size in model_sizes:
        print(f"\nğŸ“Š æ¨¡å‹å¤§å°: {input_size} -> {output_size}")
        
        # åˆ›å»ºæ¨¡å‹å’Œè¾“å…¥
        model = torch.nn.Sequential(
            torch.nn.Linear(input_size, output_size),
            torch.nn.ReLU(),
            torch.nn.Linear(output_size, output_size // 2)
        )
        
        input_data = torch.randn(1, input_size)
        
        # CPU æµ‹è¯•
        model_cpu = model
        start_time = time.time()
        output_cpu = model_cpu(input_data)
        cpu_time = time.time() - start_time
        
        # MPS æµ‹è¯•
        if torch.backends.mps.is_available():
            model_mps = model.to('mps')
            input_mps = input_data.to('mps')
            
            start_time = time.time()
            output_mps = model_mps(input_mps)
            mps_time = time.time() - start_time
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = cpu_time / mps_time
            print(f"  CPU è€—æ—¶: {cpu_time:.3f}ç§’")
            print(f"  MPS è€—æ—¶: {mps_time:.3f}ç§’")
            print(f"  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1:
                print(f"  âœ… MPS æ›´å¿«")
            else:
                print(f"  âš ï¸  CPU æ›´å¿«")
        else:
            print("  âŒ MPS ä¸å¯ç”¨")

def benchmark_convolution():
    """åŸºå‡†æµ‹è¯•å·ç§¯è¿ç®—"""
    print("\nğŸ”„ å·ç§¯è¿ç®—æ€§èƒ½æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„å·ç§¯
    conv_sizes = [
        (1, 3, 64, 64),    # å°å›¾åƒ
        (1, 3, 128, 128),  # ä¸­ç­‰å›¾åƒ
        (1, 3, 256, 256),  # å¤§å›¾åƒ
        (1, 3, 512, 512)   # è¶…å¤§å›¾åƒ
    ]
    
    for batch, channels, height, width in conv_sizes:
        print(f"\nğŸ“Š å·ç§¯å¤§å°: {batch}x{channels}x{height}x{width}")
        
        # åˆ›å»ºè¾“å…¥å’Œå·ç§¯å±‚
        input_data = torch.randn(batch, channels, height, width)
        conv_layer = torch.nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1)
        
        # CPU æµ‹è¯•
        start_time = time.time()
        output_cpu = conv_layer(input_data)
        cpu_time = time.time() - start_time
        
        # MPS æµ‹è¯•
        if torch.backends.mps.is_available():
            input_mps = input_data.to('mps')
            conv_mps = conv_layer.to('mps')
            
            start_time = time.time()
            output_mps = conv_mps(input_mps)
            mps_time = time.time() - start_time
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = cpu_time / mps_time
            print(f"  CPU è€—æ—¶: {cpu_time:.3f}ç§’")
            print(f"  MPS è€—æ—¶: {mps_time:.3f}ç§’")
            print(f"  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1:
                print(f"  âœ… MPS æ›´å¿«")
            else:
                print(f"  âš ï¸  CPU æ›´å¿«")
        else:
            print("  âŒ MPS ä¸å¯ç”¨")

def benchmark_memory_operations():
    """åŸºå‡†æµ‹è¯•å†…å­˜æ“ä½œ"""
    print("\nğŸ’¾ å†…å­˜æ“ä½œæ€§èƒ½æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„å¼ é‡æ“ä½œ
    sizes = [1000000, 5000000, 10000000, 20000000]  # å…ƒç´ æ•°é‡
    
    for size in sizes:
        print(f"\nğŸ“Š å¼ é‡å¤§å°: {size:,} å…ƒç´ ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(size)
        y = torch.randn(size)
        
        # CPU æµ‹è¯•
        start_time = time.time()
        z_cpu = x + y
        cpu_time = time.time() - start_time
        
        # MPS æµ‹è¯•
        if torch.backends.mps.is_available():
            x_mps = x.to('mps')
            y_mps = y.to('mps')
            
            start_time = time.time()
            z_mps = x_mps + y_mps
            mps_time = time.time() - start_time
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = cpu_time / mps_time
            print(f"  CPU è€—æ—¶: {cpu_time:.3f}ç§’")
            print(f"  MPS è€—æ—¶: {mps_time:.3f}ç§’")
            print(f"  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1:
                print(f"  âœ… MPS æ›´å¿«")
            else:
                print(f"  âš ï¸  CPU æ›´å¿«")
        else:
            print("  âŒ MPS ä¸å¯ç”¨")

def overall_summary():
    """æ€»ä½“æ€§èƒ½æ€»ç»“"""
    print("\nğŸ“ˆ æ€§èƒ½æ€»ç»“")
    print("=" * 40)
    
    # æ£€æµ‹è®¾å¤‡ä¿¡æ¯
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"ğŸ¯ å½“å‰è®¾å¤‡: {device}")
    
    if device.type == "mps":
        print("âœ… MPS è®¾å¤‡å¯ç”¨")
        print("ğŸ’¡ æ€§èƒ½å»ºè®®:")
        print("   - å¯¹äºå¤§å‹çŸ©é˜µè¿ç®—ï¼ŒMPS é€šå¸¸æ¯” CPU å¿« 2-5 å€")
        print("   - å¯¹äºå°å‹è¿ç®—ï¼ŒCPU å¯èƒ½æ›´å¿«ï¼ˆæ•°æ®ä¼ è¾“å¼€é”€ï¼‰")
        print("   - å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒMPS é€šå¸¸æœ‰æ˜æ˜¾ä¼˜åŠ¿")
        print("   - å»ºè®®æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©æœ€ä¼˜è®¾å¤‡")
    else:
        print("âŒ MPS è®¾å¤‡ä¸å¯ç”¨")
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   - ä¸æ˜¯ Apple Silicon Mac")
        print("   - macOS ç‰ˆæœ¬ä½äº 12.3")
        print("   - PyTorch ç‰ˆæœ¬ä¸æ”¯æŒ MPS")

if __name__ == "__main__":
    print("ğŸš€ Apple Silicon æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    benchmark_matrix_operations()
    benchmark_model_inference()
    benchmark_convolution()
    benchmark_memory_operations()
    overall_summary()
    
    print("\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆï¼") 